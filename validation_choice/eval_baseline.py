import argparse
from openai import OpenAI
import json
from tqdm import tqdm
import re
import random
from transformers import AutoTokenizer
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score

from src.batch_script import *


PROMPT = """
你要回答一道选择题，要求如下：
1. [最终选项]要从choice_1、choice_2、choice_3、choice_4选择一个，禁止回答其他内容
2. 必须按照以下格式，请先尝试回答问题，再从choice_1、choice_2、choice_3、choice_4选择一个最贴切的回答，例如：

问题：
{{
    "question": "依据《商业银行委托贷款管理办法》，以下哪项是商业银行受托发放的贷款资金用途所禁止的行为？",
    "choice_1": "支持地方金融监管部门实施持续有效的监管安排，并接受中央金融监管部门督导",
    "choice_2": "从事债券、期货、金融衍生品、资产管理产品等投资",
    "choice_3": "发放无明确场景依托和指定用途的网络小额贷款，同时防范借款人‘以贷养贷’的风险",
    "choice_4": "用于校园贷或首付贷业务，避免多头借贷现象的发生"
}}
你的回答：
[解析]:《商业银行委托贷款管理办法》明确规定，委托贷款资金不得用于债券、期货、金融衍生品等高风险投资，也不得用于法律法规和监管政策禁止的其他用途。因此，“从事债券、期货、金融衍生品、资产管理产品等投资”是被明确禁止的用途。
[最终选项]: choice_2

问题：
{{
    "question": "依据相关法规，同业代付业务主要适用于哪种情形？",
    "choice_1": "同业代付原则上仅适用于银行业金融机构办理跨境贸易结算。",
    "choice_2": "投融资性同业银行结算账户可由存款银行支行开立，并允许在异地使用，与同业代付业务相关。",
    "choice_3": "境内信用证及保理等贸易结算通常应通过支付系统或本行分支机构完成，但在同一市、县无分支机构时，可委托当地其他金融机构代付。",
    "choice_4": "同业代付业务在境内信用证结算中，可通过委托其他金融机构的方式实现变相融资功能。",
}}
你的回答：
[解析]: 问题实质在于考查对“同业代付业务适用范围”的理解。依据相关法规，同业代付业务原则上适用于跨境贸易结算，并不应被滥用于境内业务中，特别是不得作为融资手段。
[最终选项]: choice_4

问题：
{{
    "question": "依据《商业银行流动性风险管理办法》，以下关于商业银行流动性风险管理的陈述哪一项是正确的？",
    "choice_1": "在计算并表流动性覆盖率时，商业银行可以将受跨境或跨机构流动性转移限制的附属机构的所有合格优质流动性资产计入集团整体资产。",
    "choice_2": "商业银行在处理银信通道业务时，应根据其实际业务性质进行风险管控，不得借助信托通道掩盖真实风险。",
    "choice_3": "商业银行应当在法人和集团层面，分别计算未并表和并表的流动性风险监管指标，并表范围按照银监会关于商业银行资本监管的相关规定执行。",
    "choice_4": "商业银行可通过信托通道将表内资产移至表外，以此规避流动性风险监管指标的计算。",
}}
你的回答：
[解析]:《商业银行流动性风险管理办法》规定，商业银行应在法人和集团层面分别计算未并表和并表的流动性风险监管指标，并表范围按照监管机构关于商业银行资本监管的相关规定执行。
[最终选项]: choice_3

问题：
{question}

你的回答：
""" 

def parseOutput(output: str) -> str:
    """Parse the output of the model and return the answer.
    
    Args:
        output (str): The output generated by the model.

    Returns:
        str: The answer extracted from the output.
    
    """
    cleaned_text = output

    match = re.search(r"\[最终选项\]:\s*(\w+)", cleaned_text)

    if match:
        tmp = match.group(1)  # 提取匹配的内容

        if tmp in ["choice_1", "choice_2", "choice_3", "choice_4"]:
            res = tmp
        else:
            res = None
    else:
        res = None

    return res


def shuffle_choices(data):
    """Shuffle the choices in the data.

    Args:
        data (list): The input data.

    Returns:
        list: The shuffled data.
    """

    for d in data:
        answer = d['choice_1']
        choices = [d['choice_1'], d['choice_2'], d['choice_3'], d['choice_4']]
        random.shuffle(choices)
        for c in range(4):
            choice = choices[c]
            d['choice_' + str(c+1)] = choice
            if choice == answer:
                d['answer'] = f"choice_{c+1}"

    return data


def calculate_metrics(predicted_choices, actual_choices):
    choice_to_int = {'choice_1': 0, 'choice_2': 1, 'choice_3': 2, 'choice_4': 3}
    
    predicted = [choice_to_int[choice] for choice in predicted_choices]
    actual = [choice_to_int[choice] for choice in actual_choices]
    
    precision = precision_score(actual, predicted, average='macro')
    recall = recall_score(actual, predicted, average='macro')
    f1 = f1_score(actual, predicted, average='macro')
    accuracy = accuracy_score(actual, predicted)
    
    return accuracy, precision, recall, f1


def baseline_Qwen(questions: list[str], answers: list[str], type: list[str], apiKey: str) -> list:
    """Call API for question generation.
    
    Args:
        client (OpenAI): The OpenAI client.
        question (list[str]): The list of questions.

    Returns:
        list: The generated answers.

    """

    apiKey = apiKey
    client = OpenAI(
        api_key=apiKey,
        base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
    )

    res = []
    dumpList = []
    id2qa = {}
    cnt = 0
    for cnt in tqdm(range(len(questions)), desc="Qwen Generating answers ...", unit="item"):
        question_tmp = questions[cnt]
        question_tmp.pop('answer')

        truth = answers[cnt]
        the_type = type[cnt]

        id2qa[str(cnt)] = {
            "question": question_tmp,
            "truth": truth,
            "type": the_type
        }

        content = PROMPT.format(question=question_tmp)
        message = [
            {'role': 'system', 'content': '你是一个法律问答专家'},
            {'role': 'user', 'content': content}
        ]


        singleDict = {
            "custom_id": f"request-{cnt}", 
            "method": "POST", 
            "url": "/v1/chat/completions", 
            "body": {
                "model": "qwen-plus", 
                "messages": message
            }
        }

        dumpList.append(singleDict)

    with open("./messages/message_qwen.jsonl", 'w', encoding='utf-8') as f:
        for dict_item in dumpList:
            f.write(json.dumps(dict_item, ensure_ascii=False) + '\n')


    # raise Exception("Stop here")

    # call batch API
    client = OpenAI(
        api_key=apiKey,
        base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"  # 百炼服务的base_url
    )
    inputFileID = upload_file(client, "./messages/message_qwen.jsonl")
    batchID = create_batch_job(client, inputFileID)
    cnt = 0
    while True:
        output_file_id = get_output_id(client, batchID)
        if output_file_id:
            break
        else:
            print(f"[INFO]: {cnt} min waiting")
            time.sleep(60)
            cnt += 1
            
    content = client.files.content(file_id=output_file_id)
    responses = content.text.split("\n")

    waste = 0
    for r in responses:

        if r == "" or not r:
            waste += 1
            continue

        response = json.loads(r)
        text = response['response']['body']['choices'][0]['message']['content']
        text_id = response['custom_id'].split("-")[1]
        question, truth, the_type = id2qa[text_id]["question"], id2qa[text_id]["truth"], id2qa[text_id]["type"]

        answer = parseOutput(text)

        res.append({
            "question": question,
            "answer": answer,
            "truth": truth,
            "type": the_type
        })

    print(f"waste: {waste}")

    return res

def baseline_Deepseek(questions: list[str], answers: list[str], apiKey: str) -> list:
    """Call API for question generation.
    
    Args:
        client (OpenAI): The OpenAI client.
        question (list[str]): The list of questions.

    Returns:
        list: The generated answers.

    """


    apiKey = apiKey
    client = OpenAI(
        api_key=apiKey,
        base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
    )

    res = []
    dumpList = []
    id2qa = {}
    cnt = 0
    for cnt in tqdm(range(len(questions)), desc="Deepseek Generating answers ...", unit="item"):
        question_tmp = questions[cnt]
        question_tmp.pop('answer')
        
        truth = answers[cnt]
        the_type = type[cnt]

        id2qa[str(cnt)] = {
            "question": question_tmp,
            "truth": truth,
            "type": the_type
        }

        content = PROMPT.format(question=question_tmp)
        message = [
            {'role': 'system', 'content': '你是一个法律问答专家'},
            {'role': 'user', 'content': content}
        ]

        singleDict = {
            "custom_id": f"request-{cnt}", 
            "method": "POST", 
            "url": "/v1/chat/completions", 
            "body": {
                "model": "deepseek-v3", 
                "messages": message
            }
        }

        dumpList.append(singleDict)

    with open("./messages/message_deepseek.jsonl", 'w', encoding='utf-8') as f:
        for dict_item in dumpList:
            f.write(json.dumps(dict_item, ensure_ascii=False) + '\n')

    # call batch API
    client = OpenAI(
        api_key=apiKey,
        base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
    )
    inputFileID = upload_file(client, "./messages/message_deepseek.jsonl")
    batchID = create_batch_job(client, inputFileID)
    cnt = 0
    while True:
        output_file_id = get_output_id(client, batchID)
        if output_file_id:
            break
        else:
            print(f"[INFO]: {cnt} min waiting")
            time.sleep(60)
            cnt += 1
            
    content = client.files.content(file_id=output_file_id)
    responses = content.text.split("\n")

    waste = 0
    for r in responses:

        if r == "" or not r:
            waste += 1
            continue

        response = json.loads(r)
        text = response['response']['body']['choices'][0]['message']['content']
        text_id = response['custom_id'].split("-")[1]
        question, truth, the_type = id2qa[text_id]["question"], id2qa[text_id]["truth"], id2qa[text_id]["type"]

        answer = parseOutput(text)

        res.append({
            "question": question,
            "answer": answer,
            "truth": truth,
            "type": the_type
        })

    print(f"waste: {waste}")

    return res


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--model_name", type=str, help="The name or path of the pre-trained model.")
    parser.add_argument("--bert_path", type=str, help="The path of the BERT model.")
    parser.add_argument("--rawQCA_path", type=str, help="Path of raw QCA dataset")
    parser.add_argument("--output_path", type=str, help="Path of output generated questions-answers")
    parser.add_argument("--batch_size", type=int, default=8, help="The batch size for inference.")
    parser.add_argument("--api_key", type=str, help="The API key for the model.")
    parser.add_argument("--top_p", type=float, default=0.9, help="The top-p value for sampling.")
    parser.add_argument("--temperature", type=float, default=0.2, help="The temperature value for sampling.")
    parser.add_argument("--top_k", type=int, default=10, help="The top-k value for sampling.")
    args = parser.parse_args()

    # loading original data
    # Load the input data
    with open(args.rawQCA_path, "r") as f:
        data = json.load(f)

    # shuffle the choices
    shuffle_choices(data)

    # initialize the output file
    with open(args.output_path, "w", encoding='utf-8') as f:
        json.dump([], f, ensure_ascii=False, indent=4)

    # Perform batch inference of baseline model
    baseline_functions = [baseline_Qwen]

    for baseline_function in baseline_functions:

        questions = [d for d in data]
        answers = [d["answer"] for d in data]
        types = [d["type"] for d in data]

        # Perform batch inference of baseline model
        qatDictList = baseline_function(questions, answers, types, args.api_key)

        with open(args.output_path, "r", encoding='utf-8') as f:
            oriDictList = json.load(f)
        
        oriDictList.extend(qatDictList)

        with open(args.output_path, "w", encoding='utf-8') as f:
            json.dump(oriDictList, f, ensure_ascii=False, indent=4)

        res_truths = []
        res_answers = []
        res_types = []
        for qatDict in qatDictList:
            if qatDict["answer"] is None:
                continue
            res_truths.append(qatDict["truth"])
            res_answers.append(qatDict["answer"])
            res_types.append(qatDict["type"])
        
        acc, precision, recall, f1 = calculate_metrics(res_answers, res_truths)
        
        unique_types = set(res_types)
        for t in unique_types:
            t_answers = [res_answers[i] for i in range(len(res_types)) if res_types[i] == t]
            t_truths = [res_truths[i] for i in range(len(res_types)) if res_types[i] == t]
            acc_type, precision_type, recall_type, f1_type = calculate_metrics(t_answers, t_truths)
            print("=" * 200)
            print(f"Type {t} Metrics: ")
            print(f"Accuracy: {acc_type:.4f}, Precision: {precision_type:.4f}, Recall: {recall_type:.4f}, F1: {f1_type:.4f}")
            print("=" * 200)
        
        print("=" * 200)
        print("Total Metrics: ")
        print(f"Accuracy: {acc:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}")
        print("=" * 200)
        

if __name__ == "__main__":
    main()
