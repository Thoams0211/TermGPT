from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
import re


PROMPT = """
你要回答一道选择题，要求如下：
1. [最终选项]要从choice_1、choice_2、choice_3、choice_4选择一个，禁止回答其他内容
2. 必须按照以下格式，请先尝试回答问题，再从choice_1、choice_2、choice_3、choice_4选择一个最贴切的回答，例如：

问题：
{{
    "question": "依据《商业银行委托贷款管理办法》，以下哪项是商业银行受托发放的贷款资金用途所禁止的行为？",
    "choice_1": "支持地方金融监管部门实施持续有效的监管安排，并接受中央金融监管部门督导",
    "choice_2": "从事债券、期货、金融衍生品、资产管理产品等投资",
    "choice_3": "发放无明确场景依托和指定用途的网络小额贷款，同时防范借款人‘以贷养贷’的风险",
    "choice_4": "用于校园贷或首付贷业务，避免多头借贷现象的发生"
}}
你的回答：
[解析]:《商业银行委托贷款管理办法》明确规定，委托贷款资金不得用于债券、期货、金融衍生品等高风险投资，也不得用于法律法规和监管政策禁止的其他用途。因此，“从事债券、期货、金融衍生品、资产管理产品等投资”是被明确禁止的用途。
[最终选项]: choice_2

问题：
{{
    "question": "依据相关法规，同业代付业务主要适用于哪种情形？",
    "choice_1": "同业代付原则上仅适用于银行业金融机构办理跨境贸易结算。",
    "choice_2": "投融资性同业银行结算账户可由存款银行支行开立，并允许在异地使用，与同业代付业务相关。",
    "choice_3": "境内信用证及保理等贸易结算通常应通过支付系统或本行分支机构完成，但在同一市、县无分支机构时，可委托当地其他金融机构代付。",
    "choice_4": "同业代付业务在境内信用证结算中，可通过委托其他金融机构的方式实现变相融资功能。",
}}
你的回答：
[解析]: 问题实质在于考查对“同业代付业务适用范围”的理解。依据相关法规，同业代付业务原则上适用于跨境贸易结算，并不应被滥用于境内业务中，特别是不得作为融资手段。
[最终选项]: choice_4

问题：
{{
    "question": "依据《商业银行流动性风险管理办法》，以下关于商业银行流动性风险管理的陈述哪一项是正确的？",
    "choice_1": "在计算并表流动性覆盖率时，商业银行可以将受跨境或跨机构流动性转移限制的附属机构的所有合格优质流动性资产计入集团整体资产。",
    "choice_2": "商业银行在处理银信通道业务时，应根据其实际业务性质进行风险管控，不得借助信托通道掩盖真实风险。",
    "choice_3": "商业银行应当在法人和集团层面，分别计算未并表和并表的流动性风险监管指标，并表范围按照银监会关于商业银行资本监管的相关规定执行。",
    "choice_4": "商业银行可通过信托通道将表内资产移至表外，以此规避流动性风险监管指标的计算。",
}}
你的回答：
[解析]:《商业银行流动性风险管理办法》规定，商业银行应在法人和集团层面分别计算未并表和并表的流动性风险监管指标，并表范围按照监管机构关于商业银行资本监管的相关规定执行。
[最终选项]: choice_3

问题：
{question}

你的回答：
""" 


PROMPT_llama = """
你要回答一道选择题，要求如下：
1. [最终选项]要从choice_1、choice_2、choice_3、choice_4选择一个，禁止回答其他内容
2. [解析]不得超过一句话
3. 必须按照以下格式回答：

问题：
{{
    "question": "依据《商业银行委托贷款管理办法》，以下哪项是商业银行受托发放的贷款资金用途所禁止的行为？",
    "choice_1": "支持地方金融监管部门实施持续有效的监管安排，并接受中央金融监管部门督导",
    "choice_2": "从事债券、期货、金融衍生品、资产管理产品等投资",
    "choice_3": "发放无明确场景依托和指定用途的网络小额贷款，同时防范借款人‘以贷养贷’的风险",
    "choice_4": "用于校园贷或首付贷业务，避免多头借贷现象的发生"
}}
你的回答：
[最终选项]: choice_2
[解析]:《商业银行委托贷款管理办法》明确规定，委托贷款资金不得用于债券、期货、金融衍生品等高风险投资，也不得用于法律法规和监管政策禁止的其他用途。


问题：
{{
    "question": "依据相关法规，同业代付业务主要适用于哪种情形？",
    "choice_1": "同业代付原则上仅适用于银行业金融机构办理跨境贸易结算。",
    "choice_2": "投融资性同业银行结算账户可由存款银行支行开立，并允许在异地使用，与同业代付业务相关。",
    "choice_3": "境内信用证及保理等贸易结算通常应通过支付系统或本行分支机构完成，但在同一市、县无分支机构时，可委托当地其他金融机构代付。",
    "choice_4": "同业代付业务在境内信用证结算中，可通过委托其他金融机构的方式实现变相融资功能。",
}}
你的回答：
[最终选项]: choice_4
[解析]: 依据相关法规，同业代付业务原则上适用于跨境贸易结算，并不应被滥用于境内业务中，特别是不得作为融资手段。


问题：
{{
    "question": "依据《商业银行流动性风险管理办法》，以下关于商业银行流动性风险管理的陈述哪一项是正确的？",
    "choice_1": "在计算并表流动性覆盖率时，商业银行可以将受跨境或跨机构流动性转移限制的附属机构的所有合格优质流动性资产计入集团整体资产。",
    "choice_2": "商业银行在处理银信通道业务时，应根据其实际业务性质进行风险管控，不得借助信托通道掩盖真实风险。",
    "choice_3": "商业银行应当在法人和集团层面，分别计算未并表和并表的流动性风险监管指标，并表范围按照银监会关于商业银行资本监管的相关规定执行。",
    "choice_4": "商业银行可通过信托通道将表内资产移至表外，以此规避流动性风险监管指标的计算。",
}}
你的回答：
[最终选项]: choice_3
[解析]:《商业银行流动性风险管理办法》规定，商业银行应在法人和集团层面分别计算未并表和并表的流动性风险监管指标，并表范围按照监管机构关于商业银行资本监管的相关规定执行。

现在请你回答问题, [最终选项]要从choice_1、choice_2、choice_3、choice_4选择一个，禁止回答其他内容：
{question}

你的回答：
""" 


def parseOutput(output: str) -> str:
    """Parse the output of the model and return the answer.
    
    Args:
        output (str): The output generated by the model.

    Returns:
        str: The answer extracted from the output.
    
    """

    if len(output.split("你的回答：")) > 5:
        cleaned_text = output.split("你的回答：")[4].strip()
    else:
        cleaned_text = output.split("你的回答：")[-1].strip()

    match = re.search(r"\[(?:最终选项|final answer|final option|final选项)\][:：]\s*(\w+)(?=[^\n]*$)", cleaned_text)

    if match:
        tmp = match.group(1)  # 提取匹配的内容
        if tmp in ["choice_1", "choice_2", "choice_3", "choice_4"]:
            res = tmp
            return res
        elif tmp in ["1", "2", "3", "4"]:
            res = f"choice_{tmp}"
            return res
        elif tmp in ["choice1", "choice2", "choice3", "choice4"]:
            res = f"choice_{tmp[-1]}"
            return res
        else:
            res = None
            return res
    
    match = re.search(r"\[(?:最终选项|final answer|final option|final选项)\][:：]\s*(choice_\d+)", cleaned_text)
    if match:
        tmp = match.group(1)
        if tmp in ["1", "2", "3", "4"]:
            res = f"choice_{tmp}"
            return res
        elif tmp in ["choice_1", "choice_2", "choice_3", "choice_4"]:
            res = tmp
            return res
        else:
            res = None
            return res
    
    if cleaned_text in ["choice_1", "choice_2", "choice_3", "choice_4"]:
        res = cleaned_text
        return res
    elif cleaned_text in ["1", "2", "3", "4"]:
        res = f"choice_{cleaned_text}"
        return res
    else:
        res = None
        print("=" * 80)
        print (f"无法解析回答: ")
        print(cleaned_text)
        print("=" * 80)

    return res


def parseOutput_lawyer(output: str) -> str:
    print(output)

    print("=" * 80)

    return output



def batch_inference(model, tokenizer, questions, max_length=128, do_sample=True, top_p=0.9, temperature=0.2, top_k=10):
    """
    Batch inference for a list of questions.

    Args:
        questions (list of str): questions to be answered   
        max_length (int): maximum length of the generated text (default is 128)
        do_sample (bool): if set to False greedy decoding is used (default is True)
        top_p (float): nucleus sampling (top-p) (default is 0.9)
        temperature (float): temperature for sampling (default is 0.2)
        top_k (int): top-k sampling (default is 10)

    Returns:
        list: the generated answers for the questions
    """

    # Record the errors
    error_list = []

    questions_tmp = []
    for question in questions:
        questions_tmp.append({k: v for k, v in question.items() if k != "answer" and k != "type"})

    # Formmat prompt
    input_texts = [PROMPT.format(question=q) for q in questions_tmp]


    # Tokenize the input texts
    tokenizer.pad_token = tokenizer.eos_token
    inputs = tokenizer(input_texts, padding=True, truncation=True, return_tensors="pt")
    inputs = inputs.to('cuda')

    # generate the outputs
    outputs = model.generate(**inputs, max_length=max_length, do_sample=do_sample, top_p=top_p, temperature=temperature, top_k=top_k)
    output_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]

    # parse the output texts
    waste = 0
    res_output = []
    for i in range(len(output_texts)):

        output_tmp = output_texts[i]

        output_texts[i] = parseOutput(output_tmp)

        if output_texts[i] is None:
            waste += 1
            res_output.append(None)
            error_list.append(questions[i])
        else:
            res_output.append(output_texts[i])

    print(f"有{waste}个问题没有回答")

    return res_output, error_list


def batch_inference_vllm(model, tokenizer, questions, max_length=128, do_sample=True, top_p=0.9, temperature=0.2, top_k=10):
    """Batch inference for a list of questions using vLLM.
    
    Args:
        model (vLLM model): The vLLM model to use for inference.
        tokenizer (vLLM tokenizer): The tokenizer to use for encoding the questions.
        questions (list of dict): List of questions, each question is a dictionary containing the question and choices.
        max_length (int): Maximum length of the generated text (default is 128).
        do_sample (bool): If set to False, greedy decoding is used (default is True).
        top_p (float): Nucleus sampling (top-p) (default is 0.9).
        temperature (float): Temperature for sampling (default is 0.2).
        top_k (int): Top-k sampling (default is 10).
    
    Returns:
        list: The generated answers for the questions.
    
    """
   
    # Prepare the questions by removing the "answer" key
    questions_tmp = []
    for question in questions:
        questions_tmp.append({k: v for k, v in question.items() if k != "answer"})
    
    input_texts = [PROMPT_llama.format(question=q) for q in questions_tmp]
    
    # Convert input texts to vLLM input format
    sampling_params = SamplingParams(
        max_tokens=max_length,
        temperature=temperature if do_sample else 0.0,  # 0.0表示贪婪解码
        top_p=top_p if do_sample else 1.0,
        top_k=top_k if do_sample else -1,  # -1表示禁用top-k
        stop_token_ids=[tokenizer.eos_token_id]  # 添加停止token
    )
    
    outputs = model.generate(input_texts, sampling_params)
    
    # Parse the outputs
    waste = 0
    res_output = []
    
    for i, output in enumerate(outputs):
        output_tmp = output.outputs[0].text
        parsed_output = parseOutput(output_tmp)
        
        if parsed_output is None:
            waste += 1
            res_output.append(None)
        else:
            res_output.append(parsed_output)
            
    print(f"有{waste}个问题没有回答")
    
    return res_output

if __name__ == "__main__":
    pass

    



    
