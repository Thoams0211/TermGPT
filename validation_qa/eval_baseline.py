# -*- coding: utf-8 -*-
import argparse
from posix import CLD_CONTINUED
from openai import OpenAI
import json
from tqdm import tqdm
import re
import random
from transformers import AutoTokenizer

from src.util import *
from src.batch_script import *


PROMPT = """
请你直接回答以下问题：
{question}
    
必须遵照以下格式：
[答案] ... [结束]

你的回答：
""" 

def parseOutput(output: str) -> str:
    """Parse the output of the model and return the answer.
    
    Args:
        output (str): The output generated by the model.

    Returns:
        str: The answer extracted from the output.
    
    """

    cleaned_text = output

    match = re.search(r"\[答案\](.*?)\[结束\]", cleaned_text, re.DOTALL)

    if match:
        res = match.group(1).strip()
    else:
        second_match = re.search(r"\[答案\](.*?)", cleaned_text, re.DOTALL)
        if second_match:
            res = cleaned_text.split("[答案] ")[1].strip()
            print(f"res: {res}, cleaned_text: {cleaned_text}")
        else:
            res = None
            print(f"cleaned_text: {cleaned_text}")

    return res

def baseline_Qwen(questions: list[str], answers: list[str], apiKey: str) -> list:
    """Call API for question generation.
    
    Args:
        client (OpenAI): The OpenAI client.
        question (list[str]): The list of questions.

    Returns:
        list: The generated answers.

    """

    client = OpenAI(
        api_key=apiKey,
        base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
    )

    res = []
    dumpList = []
    id2qa = {}
    cnt = 0
    for cnt in tqdm(range(len(questions)), desc="Qwen Generating answers ...", unit="item"):
        question = questions[cnt]
        truth = answers[cnt]

        id2qa[str(cnt)] = {
            "question": question,
            "truth": truth
        }

        content = PROMPT.format(question=question)
        message = [
            {'role': 'system', 'content': '你是一个法律问答专家'},
            {'role': 'user', 'content': content}
        ]

        singleDict = {
            "custom_id": f"request-{cnt}", 
            "method": "POST", 
            "url": "/v1/chat/completions", 
            "body": {
                "model": "qwen-plus", 
                "messages": message
            }
        }

        dumpList.append(singleDict)

    with open("./messages/message_qwen.jsonl", 'w', encoding='utf-8') as f:
        for dict_item in dumpList:
            f.write(json.dumps(dict_item, ensure_ascii=False) + '\n')

    # call batch API
    client = OpenAI(
        api_key=apiKey,
        base_url="https://dashscope.aliyuncs.com/compatible-mode/v1" 
    )
    inputFileID = upload_file(client, "./messages/message_qwen.jsonl")
    batchID = create_batch_job(client, inputFileID)
    cnt = 0
    while True:
        output_file_id = get_output_id(client, batchID)
        if output_file_id:
            break
        else:
            print(f"[INFO]: {cnt} min waiting")
            time.sleep(60)
            cnt += 1
            
    content = client.files.content(file_id=output_file_id)
    responses = content.text.split("\n")

    for r in responses:
        if r == "" or not r:
            CLD_CONTINUED
        response = json.loads(r)
        text = response['response']['body']['choices'][0]['message']['content']
        text_id = response['custom_id'].split("-")[1]
        question, truth = id2qa[text_id]["question"], id2qa[text_id]["truth"]

        answer = parseOutput(text)

        res.append({
            "question": question,
            "answer": answer,
            "truth": truth
        })

    return res

def baseline_Deepseek(questions: list[str], answers: list[str], apiKey: str) -> list:
    """Call API for question generation.
    
    Args:
        client (OpenAI): The OpenAI client.
        question (list[str]): The list of questions.

    Returns:
        list: The generated answers.

    """


    client = OpenAI(
        api_key=apiKey,
        base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
    )

    res = []
    dumpList = []
    id2qa = {}
    cnt = 0
    for cnt in tqdm(range(len(questions)), desc="Deepseek Generating answers ...", unit="item"):
        question = questions[cnt]
        truth = answers[cnt]

        id2qa[str(cnt)] = {
            "question": question,
            "truth": truth
        }

        content = PROMPT.format(question=question)
        message = [
            {'role': 'system', 'content': '你是一个法律问答专家'},
            {'role': 'user', 'content': content}
        ]

        singleDict = {
            "custom_id": f"request-{cnt}", 
            "method": "POST", 
            "url": "/v1/chat/completions", 
            "body": {
                "model": "deepseek-v3", 
                "messages": message
            }
        }

        dumpList.append(singleDict)

    with open("./messages/message_deepseek.jsonl", 'w', encoding='utf-8') as f:
        for dict_item in dumpList:
            f.write(json.dumps(dict_item, ensure_ascii=False) + '\n')

    # call batch API
    client = OpenAI(
        api_key=apiKey,
        base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"  # 百炼服务的base_url
    )
    inputFileID = upload_file(client, "./messages/message_deepseek.jsonl")
    batchID = create_batch_job(client, inputFileID)
    cnt = 0
    while True:
        output_file_id = get_output_id(client, batchID)
        if output_file_id:
            break
        else:
            print(f"[INFO]: {cnt} min waiting")
            time.sleep(60)
            cnt += 1
            
    content = client.files.content(file_id=output_file_id)
    responses = content.text.split("\n")

    for r in responses:
        if r == "" or not r:
            continue
        response = json.loads(r)
        text = response['response']['body']['choices'][0]['message']['content']
        text_id = response['custom_id'].split("-")[1]
        question, truth = id2qa[text_id]["question"], id2qa[text_id]["truth"]

        answer = parseOutput(text)

        res.append({
            "question": question,
            "answer": answer,
            "truth": truth
        })

    return res


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--model_name", type=str, help="The name or path of the pre-trained model.")
    parser.add_argument("--bert_path", type=str, help="The path of the BERT model.")
    parser.add_argument("--rawQCA_path", type=str, help="Path of raw QCA dataset")
    parser.add_argument("--output_path", type=str, help="Path of output generated questions-answers")
    parser.add_argument("--batch_size", type=int, default=8, help="The batch size for inference.")
    parser.add_argument("--api_key", type=str, help="The API key for the model.")
    parser.add_argument("--top_p", type=float, default=0.9, help="The top-p value for sampling.")
    parser.add_argument("--temperature", type=float, default=0.2, help="The temperature value for sampling.")
    parser.add_argument("--top_k", type=int, default=10, help="The top-k value for sampling.")
    args = parser.parse_args()

    # Load the input data
    with open(args.rawQCA_path, "r") as f:
        data = json.load(f)

    # initialize the output file
    with open(args.output_path, "w", encoding='utf-8') as f:
        json.dump([], f, ensure_ascii=False, indent=4)

    # Load the tokenizer
    tokenizer = AutoTokenizer.from_pretrained(args.model_name)

    # Perform batch inference of baseline model
    baseline_functions = [baseline_Qwen]

    # Call API for baseline model
    for baseline_function in baseline_functions:

        questions = [d["question"] for d in data]
        answers = [d["answer"] for d in data]

        # Perform batch inference of baseline model
        qatDictList = baseline_function(questions, answers, args.api_key)

        with open(args.output_path, "r", encoding='utf-8') as f:
            oriDictList = json.load(f)
        oriDictList.extend(qatDictList)
        with open(args.output_path, "w", encoding='utf-8') as f:
            json.dump(oriDictList, f, ensure_ascii=False, indent=4)
        
        # Calculate the BLEU, ROUGE, and BERTScore
        batch_size = args.batch_size
        batches = [qatDictList[i:i + batch_size] for i in range(0, len(qatDictList), batch_size)]
        for batch in tqdm(batches, desc=f"Evaluating", unit="item"):
            truths = [qat["truth"] for qat in batch]
            answers = [qat["answer"] for qat in batch]
            questions = [qat["question"] for qat in batch]
            
        
        # Calculate the BLEU, ROUGE, and BERTScore
        bleu_score = calculate_bleu(truths, answers, tokenizer)
        rouge_score = calculate_rouge(truths, answers, tokenizer)
        bert_score = calculate_bertscore(truths, answers, model_path=args.bert_path, device="cuda")
        meteor_score = calculate_meteor_chinese(truths, answers)
        llm_score = calculate_llm(questions, truths, answers, args.api_key)

        print(f"BLEU Score: {bleu_score}, ROUGE Score: {rouge_score}, BERTScore: {bert_score}, Meteor Score: {meteor_score}, LLM Score: {llm_score}")


if __name__ == "__main__":
    main()