# -*- coding: utf-8 -*-
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
import re


PROMPT = """
请你回答以下问题：
    {question}
    
要求:
    1. 必须直接回答问题, 必须不带任何其他注解
    2. 必须遵照以下格式：
    [答案] ... [结束]

你的回答：
"""


def parseOutput(output: str) -> str:
    """Parse the output of the model and return the answer.
    
    Args:
        output (str): The output generated by the model.

    Returns:
        str: The answer extracted from the output.
    
    """
    
    cleaned_text = output.split("[答案] ... [结束]")[1].split("你的回答：")[1].strip()

    try:
        match = re.search(r"\[答案\]\s(.*?)\s\[结束\]", cleaned_text, re.DOTALL)

        try:
            if match:
                res = match.group(1).strip()
            else:
                second_match = re.search(r"\[答案\s*\](.*?)", cleaned_text, re.DOTALL)
                if second_match:
                    res = cleaned_text.split("[答案] ")[1].strip()
                else:
                    res = None
        except:
            res = None


    except:
        try:
            cleaned_text = output.split("你的回答：")[1].strip()
            res = cleaned_text
        except:
            res = None

    

    return res


def batch_inference(model, tokenizer, questions, max_length=128, do_sample=True, top_p=0.9, temperature=0.8, top_k=10):
    """
    Batch inference for a list of questions.

    Args:
        questions (list of str): questions to be answered   
        max_length (int): maximum length of the generated text (default is 128)
        do_sample (bool): if set to False greedy decoding is used (default is True)
        top_p (float): nucleus sampling (top-p) (default is 0.9)
        temperature (float): temperature for sampling (default is 0.2)
        top_k (int): top-k sampling (default is 10)

    Returns:
        list: the generated answers for the questions
    """

    # Formmat prompt
    input_texts = [PROMPT.format(question=q) for q in questions]

    # Tokenize the input texts
    inputs = tokenizer(input_texts, padding=True, truncation=True, return_tensors="pt")

    # generate the outputs
    outputs = model.generate(**inputs, max_length=max_length, do_sample=do_sample, top_p=top_p, temperature=temperature, top_k=top_k)
    output_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]

    # parse the output texts
    for i, output in enumerate(output_texts):
        output_texts[i] = parseOutput(output)

    return output_texts



if __name__ == "__main__":
    pass